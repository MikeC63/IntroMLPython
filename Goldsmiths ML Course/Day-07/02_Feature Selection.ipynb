{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5aedae2299ee5a0a4557c40801376eaedfedaec0"
   },
   "source": [
    "# Feature Selection and Dimensionality Reduction\n",
    "\"feature selection is the process of selecting a subset of relevent features for use in model construction\".  In normal circumstances, domain knowledge plays an important role.  Unfortunately, here in the Don't Overfit II competition, we have a binary target and 300 continuous variables \"of mysterious origin\" which forces us to try automatic feature selection techniques. https://www.kaggle.com/c/dont-overfit-ii\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "66389afe343cc88341be41b2a622a0bf016fe31d"
   },
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "66389afe343cc88341be41b2a622a0bf016fe31d"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/mysterious.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* we have only 250 samples and 302 columns \n",
    "* the risk to overfit the data is high"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create our X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f9044eebf954f8cd86f0235fbb7da986b413ec00"
   },
   "outputs": [],
   "source": [
    "# prepare for modeling\n",
    "X = df.drop(['id', 'target'], axis=1)\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f9044eebf954f8cd86f0235fbb7da986b413ec00"
   },
   "source": [
    "While many algorithms (K-nearest neighbors, and logistic regression) require features to be normalized, intuitively we can think of Principle Component Analysis (PCA) as being a prime example of when normalization is important. In PCA we are interested in the components that maximize the variance. If one component (e.g. human height) varies less than another (e.g. weight) because of their respective scales (meters vs. kilos), PCA might determine that the direction of maximal variance more closely corresponds with the ‘weight’ axis, if those features are not scaled. As a change in height of one meter can be considered much more important than the change in weight of one kilogram, this is clearly incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f9044eebf954f8cd86f0235fbb7da986b413ec00"
   },
   "outputs": [],
   "source": [
    "# scaling data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1481b24d71c47a5cf33f53f139672930258172c4"
   },
   "source": [
    "## Baseline Models\n",
    "We'll use logistic regression is a good baseline as it is fast to train and predict and scales well.  We'll also use random forest.  With  its attribute feature_importances_ we can get a sense of which features are most important.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit the base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "540cef43da57fdd7ca75ef85beea250e1390459b"
   },
   "outputs": [],
   "source": [
    "lr.fit(x_train, y_train)\n",
    "rfc.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## baseline scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"linear regression train \"  , lr.score(x_train,y_train))\n",
    "print (\"Random Forest  train  \"  , rfc.score(x_train,y_train))\n",
    "print (\"linear regression test \"  , lr.score(x_test,y_test))\n",
    "print (\"Random Forest test \"  , rfc.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0f5def6ea9428fc89bcd810ef6917b781ae716a1"
   },
   "outputs": [],
   "source": [
    "#list(zip(df.columns[2:], rfc.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0f5def6ea9428fc89bcd810ef6917b781ae716a1"
   },
   "outputs": [],
   "source": [
    "featureImportance = pd.DataFrame(rfc.feature_importances_, columns=[\"importance\"])\n",
    "featureImportance = featureImportance.sort_values([\"importance\"], ascending = False) ### newer version might require sort not sort_values\n",
    "featureImportance.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "aeee1dc24f50ffa850b33f615bb1aa9fdd99ca72"
   },
   "source": [
    "# We can apply feature selection techniques to improve model performance.\n",
    "\n",
    "## 1.  Remove features with missing values\n",
    "This one is pretty self explanatory.  First we check for missing values and then can remove columns exceeding a threshold we define."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "af13dd815cb25cfbd5fe73889e45bd260d5ad6f4"
   },
   "outputs": [],
   "source": [
    "# check missing values\n",
    "df.isnull().any().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "258dc4346a8d383f7c5bf2cd67a3f77dd5826bac"
   },
   "source": [
    "The dataset has no missing values and therefore no features to remove at this step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "be8ba87e739f0c4a5b05cd6aafba0a50effdfb6e"
   },
   "source": [
    "## 2.  Remove highly correlated features\n",
    "\n",
    "Features that are highly correlated or colinear can cause overfitting.  Here we will explore correlations among features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ce29341a767d26541be03e374c50873cbff834f6"
   },
   "outputs": [],
   "source": [
    "# find correlations to target\n",
    "corr_matrix = df.corr().abs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using abs() ** the minus becoms plus so we can sort correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ce29341a767d26541be03e374c50873cbff834f6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.heatmap(corr_matrix)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# in case of correlation to find the names of the columns ... I believe there are other methods.. depending on the size of data\n",
    "\n",
    "s = corr_matrix.unstack()\n",
    "so = pd.DataFrame(s.sort_values(kind=\"quicksort\"), columns = [\"corr\"])\n",
    "so.sort_values([\"corr\"], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ce29341a767d26541be03e374c50873cbff834f6"
   },
   "source": [
    "* no much correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ce29341a767d26541be03e374c50873cbff834f6"
   },
   "source": [
    "## 3.  correlation to the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ce29341a767d26541be03e374c50873cbff834f6"
   },
   "outputs": [],
   "source": [
    "print(corr_matrix['target'].sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e08edb4a1fff3d5308d53176b7db11b015fdee1f"
   },
   "source": [
    "From the above correlation matrix we see that there are no highly correlated features in the dataset.  And even exploring correlation to target shows feature 33 with the highest correlation of only 0.37. (obviously target is 100% correlated with itself)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "adb9e665cee36e3b43a1a5f407cd18f073237c72"
   },
   "source": [
    "## 4.  Univariate Feature Selection\n",
    "We can use sklearn's SelectKBest to select a number of features to keep.  This method uses statistical tests to select features having the highest correlation to the target.  Here we will keep the top 100 features. https://scikit-learn.org/stable/auto_examples/feature_selection/plot_feature_selection.html#sphx-glr-auto-examples-feature-selection-plot-feature-selection-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d203301c1b68d2176a393b926bc7cf2210b22620"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# feature extraction\n",
    "k_best = SelectKBest( k=100)\n",
    "# fit on train set\n",
    "k_best.fit(x_train, y_train)\n",
    "# transform train set\n",
    "univariate_features = k_best.transform(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "13595023cd46efeeab2b6d1b33691b14e08b4add"
   },
   "source": [
    "\n",
    "* create the model again\n",
    "* fit the model with the selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "13595023cd46efeeab2b6d1b33691b14e08b4add"
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "rfc = RandomForestClassifier(n_estimators=100)\n",
    "lr.fit(univariate_features, y_train)\n",
    "rfc.fit(univariate_features, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "13595023cd46efeeab2b6d1b33691b14e08b4add"
   },
   "source": [
    "* we need to transform the test data as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "13595023cd46efeeab2b6d1b33691b14e08b4add"
   },
   "outputs": [],
   "source": [
    "univariate_features_test = k_best.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "13595023cd46efeeab2b6d1b33691b14e08b4add"
   },
   "outputs": [],
   "source": [
    "print (\"linear regression train \"  , lr.score(univariate_features,y_train))\n",
    "print (\"Random Forest  train  \"  , rfc.score(univariate_features,y_train))\n",
    "print (\"linear regression test \"  , lr.score(univariate_features_test,y_test))\n",
    "print (\"Random Forest test \"  , rfc.score(univariate_features_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "univariate_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b69f8de5305bab23041aaaf4b6beb71cb3c7ddcd"
   },
   "outputs": [],
   "source": [
    "featureImportance = pd.DataFrame(rfc.feature_importances_, columns=[\"importance\"])\n",
    "featureImportance = featureImportance.sort_values([\"importance\"], ascending = False) ### newer version might require sort not sort_values\n",
    "featureImportance.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8bf926f2d72b23a95afbd68778a7c6e5040768e6"
   },
   "source": [
    "## 5. PCA\n",
    "\n",
    "PCA (Principle Component Analysis) is a dimensionality reduction technique that projects the data into a lower dimensional space.\n",
    "PCA can be useful in many situations, but especially in cases with excessive multicollinearity or explanation of predictors is not a priority."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fb855ab6eb11ab8ac810251a5887c39316431b27"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# pca - keep 90% of variance\n",
    "pca = PCA(0.50)\n",
    "\n",
    "x_train_components = pca.fit_transform(x_train)\n",
    "x_train_components.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "091548127bc7531e985b1e7b3b42adc3cf162ee5"
   },
   "source": [
    "* we need to transform the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_components = pca.transform(x_test)\n",
    "x_test_components.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "091548127bc7531e985b1e7b3b42adc3cf162ee5"
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "rfc = RandomForestClassifier(n_estimators=100, max_depth=100)\n",
    "lr.fit(x_train_components, y_train)\n",
    "rfc.fit(x_train_components, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "091548127bc7531e985b1e7b3b42adc3cf162ee5"
   },
   "outputs": [],
   "source": [
    "print (\"linear regression train \"  , lr.score(x_train_components,y_train))\n",
    "print (\"Random Forest  train  \"  , rfc.score(x_train_components,y_train))\n",
    "\n",
    "print (\"linear regression test \"  , lr.score(x_test_components,y_test))\n",
    "print (\"Random Forest test \"  , rfc.score(x_test_components,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOUR TURN \n",
    "\n",
    "* try to keep the 70% of the data\n",
    "* try to use n_components=10 in the PCA parameter\n",
    "* what is the main advantage of using PCA in this case?\n",
    "* how would do you reduce the random forest accuracy in the training data?\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=1)\n",
    "\n",
    "x_train_components = pca.fit_transform(x_train)\n",
    "print(x_train_components.shape)\n",
    "\n",
    "x_test_components = pca.transform(x_test)\n",
    "print(x_test_components.shape)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "rfc = RandomForestClassifier(n_estimators=100, max_depth=3)\n",
    "lr.fit(x_train_components, y_train)\n",
    "rfc.fit(x_train_components, y_train)\n",
    "\n",
    "print (\"linear regression train \"  , lr.score(x_train_components,y_train))\n",
    "print (\"Random Forest  train  \"  , rfc.score(x_train_components,y_train))\n",
    "\n",
    "print (\"linear regression test \"  , lr.score(x_test_components,y_test))\n",
    "print (\"Random Forest test \"  , rfc.score(x_test_components,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
